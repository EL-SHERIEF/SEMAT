name: Export Framer Sites to HTML

on:
  push:
    branches:
      - main
  schedule:
    - cron: '*/20 * * * *'  # Runs every 20 minutes
  workflow_dispatch:  # Allows manual triggering of the workflow

jobs:
  export-and-commit:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout Repository
        uses: actions/checkout@v3

      - name: Install HTTrack and Dependencies
        run: sudo apt-get install -y httrack python3 curl

      - name: Export Arabic Framer Site (Main Site)
        run: |
          echo "Downloading Arabic site..."
          httrack "https://semat.framer.website/ar" -O ./main_site --mirror --keep-alive --depth=2 --clean
          if [ -d "./main_site/semat.framer.website/ar" ]; then
            rsync -a --exclude 'ar' ./main_site/semat.framer.website/ar/ ./
          fi
          rm -rf ./main_site

      - name: Export English Framer Site
        run: |
          echo "Downloading English site..."
          httrack "https://semat.framer.website/" -O ./en_site --mirror --keep-alive --depth=2 --clean
          mkdir -p ./en
          if [ -d "./en_site/semat.framer.website" ]; then
            rsync -a --exclude 'ar' ./en_site/semat.framer.website/ ./en/
          fi
          rm -rf ./en_site

      - name: Generate Sitemaps, Robots, Crawler Trap, and RSS
        run: |
          # Random offset for fake dynamic lastmod (0-60 minutes ago)
          RANDOM_OFFSET=$((RANDOM % 60))

          # Sitemap for root (Arabic)
          echo '<?xml version="1.0" encoding="UTF-8"?>' > sitemap.xml
          echo '<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">' >> sitemap.xml
          find . -maxdepth 1 -type f -name "*.html" -not -path "./en/*" | while read -r file; do
            url="https://semat.agency/${file#./}"
            lastmod=$(date -u -d "$RANDOM_OFFSET minutes ago" +%Y-%m-%dT%H:%M:%SZ)
            echo "  <url><loc>$url</loc><lastmod>$lastmod</lastmod><changefreq>daily</changefreq><priority>1.0</priority></url>" >> sitemap.xml
          done
          echo '</urlset>' >> sitemap.xml

          # Sitemap for /en (English)
          echo '<?xml version="1.0" encoding="UTF-8"?>' > en/sitemap.xml
          echo '<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">' >> en/sitemap.xml
          find ./en -type f -name "*.html" | while read -r file; do
            url="https://semat.agency/${file#./}"
            lastmod=$(date -u -d "$RANDOM_OFFSET minutes ago" +%Y-%m-%dT%H:%M:%SZ)
            echo "  <url><loc>$url</loc><lastmod>$lastmod</lastmod><changefreq>daily</changefreq><priority>1.0</priority></url>" >> en/sitemap.xml
          done
          echo '</urlset>' >> en/sitemap.xml

          # Robots.txt for root
          echo "User-agent: *" > robots.txt
          echo "Allow: /" >> robots.txt
          echo "Disallow: /en/" >> robots.txt
          echo "Allow: /crawl-me/" >> robots.txt
          echo "Sitemap: https://semat.agency/sitemap.xml" >> robots.txt

          # Robots.txt for /en
          echo "User-agent: *" > en/robots.txt
          echo "Allow: /en/" >> en/robots.txt
          echo "Disallow: /" >> en/robots.txt
          echo "Allow: /crawl-me/" >> en/robots.txt
          echo "Sitemap: https://semat.agency/en/sitemap.xml" >> en/robots.txt

          # Crawler trap in /crawl-me
          mkdir -p crawl-me
          echo "<html><body><h1>Crawler Trap</h1><ul>" > crawl-me/index.html
          find . -maxdepth 1 -type f -name "*.html" -not -path "./en/*" -not -path "./crawl-me/*" | while read -r file; do
            echo "<li><a href=\"https://semat.agency/${file#./}\">${file#./}</a></li>" >> crawl-me/index.html
          done
          find ./en -type f -name "*.html" | while read -r file; do
            echo "<li><a href=\"https://semat.agency/${file#./}\">${file#./}</a></li>" >> crawl-me/index.html
          done
          echo "</ul></body></html>" >> crawl-me/index.html

          # Fake RSS feed
          echo '<?xml version="1.0" encoding="UTF-8"?>' > rss.xml
          echo '<rss version="2.0"><channel>' >> rss.xml
          echo "<title>Semat Agency Updates</title><link>https://semat.agency</link><description>Latest updates</description>" >> rss.xml
          find . -maxdepth 1 -type f -name "*.html" -not -path "./en/*" | while read -r file; do
            url="https://semat.agency/${file#./}"
            echo "<item><title>${file#./}</title><link>$url</link><pubDate>$(date -u -R)</pubDate></item>" >> rss.xml
          done
          find ./en -type f -name "*.html" | while read -r file; do
            url="https://semat.agency/${file#./}"
            echo "<item><title>${file#./}</title><link>$url</link><pubDate>$(date -u -R)</pubDate></item>" >> rss.xml
          done
          echo '</channel></rss>' >> rss.xml

      - name: Ping Search Engines and RSS Aggregators
        run: |
          echo "Pinging search engines and RSS aggregators (Google ping deprecated)..."
          # Bing
          curl -s "http://www.bing.com/ping?sitemap=https://semat.agency/sitemap.xml"
          curl -s "http://www.bing.com/ping?sitemap=https://semat.agency/en/sitemap.xml"
          # Yahoo (via Bing)
          curl -s "http://www.bing.com/ping?sitemap=https://semat.agency/sitemap.xml"
          curl -s "http://www.bing.com/ping?sitemap=https://semat.agency/en/sitemap.xml"
          # Yandex
          curl -s "http://ping.blogs.yandex.ru/ping?sitemap=https://semat.agency/sitemap.xml"
          curl -s "http://ping.blogs.yandex.ru/ping?sitemap=https://semat.agency/en/sitemap.xml"
          # RSS pings
          curl -s "http://ping.feedburner.com/ping?url=https://semat.agency/rss.xml"
          curl -s "http://rpc.pingomatic.com/?url=https://semat.agency/rss.xml"
          echo "Ping requests sent successfully."

      - name: Commit and Push Changes
        env:
          ACTIONS_PAT: ${{ secrets.ACTIONS_PAT }}
        run: |
          git config user.name 'github-actions[bot]'
          git config user.email 'github-actions[bot]@users.noreply.github.com'
          git add .
          if git diff --cached --quiet; then
            echo "No changes to commit."
          else
            git commit -m "Automated update with dark SEO tweaks (Google ping removed)"
            git push https://x-access-token:${{ secrets.ACTIONS_PAT }}@github.com/${{ github.repository }}.git HEAD:main
          fi
