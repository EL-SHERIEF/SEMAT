name: Export Framer Sites to HTML

on:
  push:
    branches:
      - main
  schedule:
    - cron: '0 * * * *'  # Runs every hour
  workflow_dispatch:  # Allows manual triggering of the workflow

jobs:
  export-and-commit:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout Repository
        uses: actions/checkout@v3

      - name: Install HTTrack and Dependencies
        run: sudo apt-get install -y httrack python3 curl

      - name: Export Arabic Framer Site (Main Site)
        run: |
          echo "Downloading Arabic site..."
          httrack "https://semat.framer.website/ar" -O ./main_site --mirror --keep-alive --depth=3 --robots=0 --ext-depth=1 --clean
          if [ -d "./main_site/semat.framer.website/ar" ]; then
            rsync -a ./main_site/semat.framer.website/ar/ ./
            # Ensure index.html exists in the root
            if [ ! -f "./index.html" ] && [ -f "./ar/index.html" ]; then
              mv ./ar/index.html ./index.html
              # Optional: Clean up the ar directory if not needed
              rm -rf ./ar
            fi
          fi
          rm -rf ./main_site

      - name: Ensure Root index.html Exists
        run: |
          if [ ! -f "./index.html" ]; then
            echo '<html><body><h1>Welcome to Semat Agency</h1><p>Redirecting...</p><meta http-equiv="refresh" content="0;url=/ar/index.html"></body></html>' > index.html
          fi

      - name: Export English Framer Site
        run: |
          echo "Downloading English site..."
          httrack "https://semat.framer.website/" -O ./en_site --mirror --keep-alive --depth=3 --robots=0 --ext-depth=1 --clean
          mkdir -p ./en
          if [ -d "./en_site/semat.framer.website" ]; then
            rsync -a --exclude 'ar' ./en_site/semat.framer.website/ ./en/
          fi
          rm -rf ./en_site

      - name: Create Vercel Configuration
        run: |
          echo '{"rewrites": [{"source": "/:path*", "destination": "/index.html"}], "redirects": [{"source": "/:path*", "has": [{"type": "host", "value": "www.semat.agency"}], "destination": "https://semat.agency/:path*", "permanent": true}]}' > vercel.json

      - name: Debug File Structure
        run: |
          echo "Files in root:"
          ls -la .
          echo "Files in /en:"
          ls -la ./en || echo "No /en directory"
          echo "Checking for services/blog files:"
          ls -la ./services/blog || echo "No services/blog directory in root"
          ls -la ./en/services/blog || echo "No services/blog directory in /en"

      - name: Generate Sitemaps, Robots, Crawler Trap, and RSS
        run: |
          # Random offset for fake dynamic lastmod (0-60 minutes ago)
          RANDOM_OFFSET=$((RANDOM % 60))

          # Sitemap for root (Arabic) - removed maxdepth to include subdirectories
          echo '<?xml version="1.0" encoding="UTF-8"?>' > sitemap.xml
          echo '<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">' >> sitemap.xml
          find . -type f -name "*.html" -not -path "./en/*" -not -path "./crawl-me/*" | while read -r file; do
            url="https://semat.agency/${file#./}"
            lastmod=$(date -u -d "$RANDOM_OFFSET minutes ago" +%Y-%m-%dT%H:%M:%SZ)
            echo "  <url><loc>$url</loc><lastmod>$lastmod</lastmod><changefreq>daily</changefreq><priority>1.0</priority></url>" >> sitemap.xml
          done
          echo '</urlset>' >> sitemap.xml

          # Sitemap for /en (English) - already recursive, kept as is
          echo '<?xml version="1.0" encoding="UTF-8"?>' > en/sitemap.xml
          echo '<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">' >> en/sitemap.xml
          find ./en -type f -name "*.html" | while read -r file; do
            url="https://semat.agency/${file#./}"
            lastmod=$(date -u -d "$RANDOM_OFFSET minutes ago" +%Y-%m-%dT%H:%M:%SZ)
            echo "  <url><loc>$url</loc><lastmod>$lastmod</lastmod><changefreq>daily</changefreq><priority>1.0</priority></url>" >> en/sitemap.xml
          done
          echo '</urlset>' >> en/sitemap.xml

          # Robots.txt for root (consolidated)
          echo "User-agent: *" > robots.txt
          echo "Allow: /" >> robots.txt
          echo "Allow: /crawl-me/" >> robots.txt
          echo "Sitemap: https://semat.agency/sitemap.xml" >> robots.txt
          echo "Sitemap: https://semat.agency/en/sitemap.xml" >> robots.txt
          echo "Host: semat.agency" >> robots.txt

          # Crawler trap in /crawl-me - updated to include subdirectories
          mkdir -p crawl-me
          echo "<html><body><h1>Crawler Trap</h1><ul>" > crawl-me/index.html
          find . -type f -name "*.html" -not -path "./en/*" -not -path "./crawl-me/*" | while read -r file; do
            echo "<li><a href=\"https://semat.agency/${file#./}\">${file#./}</a></li>" >> crawl-me/index.html
          done
          find ./en -type f -name "*.html" | while read -r file; do
            echo "<li><a href=\"https://semat.agency/${file#./}\">${file#./}</a></li>" >> crawl-me/index.html
          done
          echo "</ul></body></html>" >> crawl-me/index.html

          # Fake RSS feed - updated to include subdirectories
          echo '<?xml version="1.0" encoding="UTF-8"?>' > rss.xml
          echo '<rss version="2.0"><channel>' >> rss.xml
          echo "<title>Semat Agency Updates</title><link>https://semat.agency</link><description>Latest updates</description>" >> rss.xml
          find . -type f -name "*.html" -not -path "./en/*" -not -path "./crawl-me/*" | while read -r file; do
            url="https://semat.agency/${file#./}"
            echo "<item><title>${file#./}</title><link>$url</link><pubDate>$(date -u -R)</pubDate></item>" >> rss.xml
          done
          find ./en -type f -name "*.html" | while read -r file; do
            url="https://semat.agency/${file#./}"
            echo "<item><title>${file#./}</title><link>$url</link><pubDate>$(date -u -R)</pubDate></item>" >> rss.xml
          done
          echo '</channel></rss>' >> rss.xml

      - name: Ping Search Engines and RSS Aggregators
        run: |
          echo "Pinging search engines and RSS aggregators (Google ping deprecated)..."

          # Track failures
          FAILURES=0

          # Bing (covers Yahoo as well)
          if curl -s "http://www.bing.com/ping?sitemap=https://semat.agency/sitemap.xml"; then
            echo "Bing (and Yahoo) ping for sitemap.xml successful"
          else
            echo "Bing (and Yahoo) ping for sitemap.xml failed" >&2
            echo "Error details: $(curl -s -o /dev/null -w '%{http_code}' 'http://www.bing.com/ping?sitemap=https://semat.agency/sitemap.xml')" >&2
            FAILURES=$((FAILURES + 1))
          fi
          if curl -s "http://www.bing.com/ping?sitemap=https://semat.agency/en/sitemap.xml"; then
            echo "Bing (and Yahoo) ping for en/sitemap.xml successful"
          else
            echo "Bing (and Yahoo) ping for en/sitemap.xml failed" >&2
            echo "Error details: $(curl -s -o /dev/null -w '%{http_code}' 'http://www.bing.com/ping?sitemap=https://semat.agency/en/sitemap.xml')" >&2
            FAILURES=$((FAILURES + 1))
          fi

          # Yandex
          if curl -s "http://ping.blogs.yandex.ru/ping?sitemap=https://semat.agency/sitemap.xml"; then
            echo "Yandex ping for sitemap.xml successful"
          else
            echo "Yandex ping for sitemap.xml failed" >&2
            echo "Error details: $(curl -s -o /dev/null -w '%{http_code}' 'http://ping.blogs.yandex.ru/ping?sitemap=https://semat.agency/sitemap.xml')" >&2
            FAILURES=$((FAILURES + 1))
          fi
          if curl -s "http://ping.blogs.yandex.ru/ping?sitemap=https://semat.agency/en/sitemap.xml"; then
            echo "Yandex ping for en/sitemap.xml successful"
          else
            echo "Yandex ping for en/sitemap.xml failed" >&2
            echo "Error details: $(curl -s -o /dev/null -w '%{http_code}' 'http://ping.blogs.yandex.ru/ping?sitemap=https://semat.agency/en/sitemap.xml')" >&2
            FAILURES=$((FAILURES + 1))
          fi

          # RSS pings
          if curl -s "http://ping.feedburner.com/ping?url=https://semat.agency/rss.xml"; then
            echo "Feedburner RSS ping successful"
          else
            echo "Feedburner RSS ping failed" >&2
            echo "Error details: $(curl -s -o /dev/null -w '%{http_code}' 'http://ping.feedburner.com/ping?url=https://semat.agency/rss.xml')" >&2
            FAILURES=$((FAILURES + 1))
          fi
          if curl -s "http://rpc.pingomatic.com/?url=https://semat.agency/rss.xml"; then
            echo "Pingomatic RSS ping successful"
          else
            echo "Pingomatic RSS ping failed" >&2
            echo "Error details: $(curl -s -o /dev/null -w '%{http_code}' 'http://rpc.pingomatic.com/?url=https://semat.agency/rss.xml')" >&2
            FAILURES=$((FAILURES + 1))
          fi

          # Final status
          if [ $FAILURES -eq 0 ]; then
            echo "All ping requests sent successfully."
          else
            echo "Completed with $FAILURES ping failures." >&2
            # Exit with failure only if more than half fail (3 out of 6)
            if [ $FAILURES -ge 3 ]; then
              exit 1
            fi
          fi

      - name: Commit and Push Changes
        env:
          ACTIONS_PAT: ${{ secrets.ACTIONS_PAT }}
        run: |
          git config user.name 'github-actions[bot]'
          git config user.email 'github-actions[bot]@users.noreply.github.com'
          git add .
          if git diff --cached --quiet; then
            echo "No changes to commit."
          else
            git commit -m "Automated update with Vercel routing and SEO tweaks"
            git push https://x-access-token:${{ secrets.ACTIONS_PAT }}@github.com/${{ github.repository }}.git HEAD:main
          fi
